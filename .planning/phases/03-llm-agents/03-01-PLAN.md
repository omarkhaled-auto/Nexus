---
phase: 03-llm-agents
plan: 01
type: tdd
depends_on: []
files_modified: [src/llm/clients/ClaudeClient.ts, src/llm/clients/ClaudeClient.test.ts, src/llm/clients/GeminiClient.ts, src/llm/clients/GeminiClient.test.ts, src/llm/LLMProvider.ts, src/llm/LLMProvider.test.ts, src/llm/types.ts, src/llm/index.ts]
domain: llm
---

<objective>
Implement LLM clients for Claude, Gemini, and unified provider interface.

Purpose: Enable agents to interact with LLM APIs. Claude for code generation (Opus for planning, Sonnet for coding/testing), Gemini for code review. This is the foundation for all agent intelligence.
Output: Working ClaudeClient, GeminiClient, and LLMProvider with streaming, retry logic, and token counting.
</objective>

<execution_context>
./.claude/get-shit-done/workflows/execute-plan.md
./.claude/get-shit-done/templates/summary.md
./.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase patterns:
@.planning/phases/01-foundation/01-04-SUMMARY.md
@.planning/phases/02-persistence/02-01-SUMMARY.md

# Master Book reference:
@07_NEXUS_MASTER_BOOK.md (Section 4.4, BUILD-008)

**Model Configuration (with Extended Thinking):**
| Agent | Model | Max Tokens | Temp | Thinking Budget |
|-------|-------|------------|------|-----------------|
| planner | claude-opus-4-5-20250514 | 16000 | 1.0 | 10000 |
| coder | claude-opus-4-5-20250514 | 16000 | 1.0 | 5000 |
| reviewer | gemini-3.0-pro (fallback: gemini-2.5-pro) | 8000 | 0.2 | - |
| tester | claude-sonnet-4-5-20250514 | 8000 | 0.3 | - |
| merger | claude-sonnet-4-5-20250514 | 4000 | 0.1 | - |

**Extended Thinking Requirements:**
- temperature=1 is REQUIRED for extended thinking mode
- Streaming is REQUIRED for extended thinking
- budget_tokens controls thinking depth (separate from output tokens)
- Thinking content returned in separate block before response

**Established patterns:**
- Custom error types with Object.setPrototypeOf
- Optional logger injection
- TDD: RED-GREEN-REFACTOR
</context>

<feature>
  <name>LLM Types and Interfaces</name>
  <files>src/llm/types.ts</files>
  <behavior>
    Message:
    - role: 'system' | 'user' | 'assistant' | 'tool'
    - content: string
    - toolCalls?: ToolCall[]
    - toolResults?: ToolResult[]

    ToolCall:
    - id: string
    - name: string
    - arguments: Record<string, unknown>

    ToolResult:
    - toolCallId: string
    - result: string | object

    ChatOptions:
    - maxTokens?: number
    - temperature?: number
    - tools?: ToolDefinition[]
    - stopSequences?: string[]
    - thinking?: ThinkingConfig

    ThinkingConfig:
    - type: 'enabled' | 'disabled'
    - budgetTokens?: number (only when type='enabled')

    ToolDefinition:
    - name: string
    - description: string
    - inputSchema: JSONSchema

    Response:
    - content: string
    - thinking?: string (extended thinking content, if enabled)
    - toolCalls?: ToolCall[]
    - usage: TokenUsage
    - finishReason: 'stop' | 'tool_use' | 'max_tokens' | 'error'

    TokenUsage:
    - inputTokens: number
    - outputTokens: number
    - thinkingTokens?: number (only with extended thinking)
    - totalTokens: number

    ModelConfig:
    - model: string
    - maxTokens: number
    - temperature: number

    AgentType: 'planner' | 'coder' | 'tester' | 'reviewer' | 'merger'
  </behavior>
  <implementation>
    Define TypeScript interfaces and types.
    Export all types for use by other modules.
  </implementation>
</feature>

<feature>
  <name>ClaudeClient - Anthropic API Client</name>
  <files>src/llm/clients/ClaudeClient.ts, src/llm/clients/ClaudeClient.test.ts</files>
  <behavior>
    Error Types:
    - LLMError: Base error
    - APIError: API returned error response
    - RateLimitError: Rate limited (429)
    - AuthenticationError: Invalid API key (401)
    - TimeoutError: Request timed out

    Constructor:
    - Accept apiKey (string)
    - Accept optional baseUrl (for testing)
    - Accept optional logger
    - Accept optional timeout (default: 60000ms)

    Methods:
    - chat(messages: Message[], options?: ChatOptions): Promise<Response>
    - chatStream(messages: Message[], options?: ChatOptions): AsyncGenerator<StreamChunk>
    - countTokens(content: string): number (approximate)

    StreamChunk:
    - type: 'thinking' | 'text' | 'tool_use' | 'done'
    - content?: string
    - toolCall?: ToolCall

    Extended Thinking:
    - When thinking.type='enabled', MUST use streaming (chatStream)
    - chat() auto-switches to stream when thinking enabled
    - Thinking content streamed before text content
    - ThinkingConfig passed to API as { thinking: { type, budget_tokens } }

    Retry Logic:
    - Retry on 429 (rate limit) with exponential backoff
    - Retry on 5xx (server errors) up to 3 times
    - No retry on 4xx (client errors) except 429

    Cases:
    - chat with simple message → Response with content
    - chat with tools → Response with toolCalls
    - chatStream yields chunks in order
    - chatStream on tool_use yields tool call chunk
    - chatStream with thinking yields thinking chunk before text
    - chat with thinking enabled auto-streams and collects result
    - thinking response includes thinkingTokens in usage
    - Rate limit triggers retry with backoff
    - Server error triggers retry
    - Client error throws immediately
    - Timeout throws TimeoutError
    - Invalid API key throws AuthenticationError
    - countTokens returns approximate count (~4 chars per token)
    - thinking with temperature != 1 throws error
  </behavior>
  <implementation>
    Use Anthropic SDK (@anthropic-ai/sdk) for API calls.
    Implement streaming with async generator.
    Exponential backoff: 1s, 2s, 4s delays.
    Parse tool use from Claude's response format.
    Extended thinking: Pass thinking config to API, handle thinking blocks in stream.
    Validate temperature=1 when thinking enabled.
  </implementation>
</feature>

<feature>
  <name>GeminiClient - Google AI API Client</name>
  <files>src/llm/clients/GeminiClient.ts, src/llm/clients/GeminiClient.test.ts</files>
  <behavior>
    Constructor:
    - Accept apiKey (string)
    - Accept optional model (default: 'gemini-3.0-pro', fallback: 'gemini-2.5-pro')
    - Accept optional logger
    - Accept optional timeout (default: 120000ms for large context)

    Methods:
    - chat(messages: Message[], options?: ChatOptions): Promise<Response>
    - chatStream(messages: Message[], options?: ChatOptions): AsyncGenerator<StreamChunk>
    - countTokens(content: string): number (approximate)

    Features:
    - Large context support (up to 1M tokens)
    - Safety settings configured for code review
    - No tool use (Gemini used only for review)

    Cases:
    - chat with code review prompt → Response with review
    - chatStream yields chunks
    - Handles large context (>100k tokens)
    - API error throws APIError
    - Rate limit triggers retry
    - Timeout throws TimeoutError
    - countTokens returns approximate count
  </behavior>
  <implementation>
    Use Google GenAI SDK (@google/genai) - new official SDK.
    Configure safety settings to allow code discussion.
    Handle large context windows (up to 1M tokens).
    Model fallback: Try gemini-3.0-pro, fallback to gemini-2.5-pro if unavailable.
  </implementation>
</feature>

<feature>
  <name>LLMProvider - Unified Provider Interface</name>
  <files>src/llm/LLMProvider.ts, src/llm/LLMProvider.test.ts</files>
  <behavior>
    Constructor:
    - Accept claudeApiKey (string)
    - Accept geminiApiKey (string)
    - Accept optional logger
    - Accept optional customConfigs (partial model configs)

    Methods:
    - getClient(agentType: AgentType): LLMClient
    - getModelConfig(agentType: AgentType): ModelConfig
    - trackUsage(agentType: AgentType, usage: TokenUsage): void
    - getUsageStats(): UsageStats

    UsageStats:
    - byAgent: Record<AgentType, { tokens: number, calls: number, cost: number }>
    - total: { tokens: number, calls: number, cost: number }

    Model Selection (with Extended Thinking):
    - planner → ClaudeClient (claude-opus-4-5, temp=1, thinking=10000)
    - coder → ClaudeClient (claude-opus-4-5, temp=1, thinking=5000)
    - tester → ClaudeClient (claude-sonnet-4-5, temp=0.3)
    - reviewer → GeminiClient (gemini-3.0-pro, fallback gemini-2.5-pro)
    - merger → ClaudeClient (claude-sonnet-4-5, temp=0.1)

    Cost Calculation (per 1M tokens):
    - claude-opus-4-5: $15 input, $75 output (thinking tokens billed as output)
    - claude-sonnet-4-5: $3 input, $15 output
    - gemini-3.0-pro: $1.25 input, $5 output
    - gemini-2.5-pro: $1.25 input, $5 output

    Cases:
    - getClient returns correct client per agent type
    - getModelConfig returns correct config
    - trackUsage accumulates stats
    - getUsageStats returns totals
    - Custom config overrides defaults
    - Cost calculated correctly
  </behavior>
  <implementation>
    Create client instances lazily (on first use).
    Store usage stats in memory.
    Calculate costs based on model pricing.
  </implementation>
</feature>

<verification>
- [ ] `pnpm test -- --grep "ClaudeClient"` - All tests pass
- [ ] `pnpm test -- --grep "GeminiClient"` - All tests pass
- [ ] `pnpm test -- --grep "LLMProvider"` - All tests pass
- [ ] `pnpm typecheck` passes
- [ ] `pnpm lint` passes for new files
</verification>

<success_criteria>
- All tests pass (target: 30+ tests)
- ClaudeClient works with streaming, tools, and extended thinking
- Extended thinking correctly streams thinking content before response
- GeminiClient works for code review with model fallback
- LLMProvider routes to correct client per agent with thinking config
- Retry logic handles rate limits
- Token counting includes thinking tokens
- Ready for BUILD-009 (Agent Runners)
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-agents/03-01-SUMMARY.md` following the summary template with TDD-specific sections.
</output>
