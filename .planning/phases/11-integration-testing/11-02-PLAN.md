---
phase: 11-integration-testing
plan: 02
type: execute
depends_on: ["11-01"]
files_modified: [tests/integration/infra-persistence.test.ts, tests/integration/persistence-planning.test.ts, tests/integration/planning-execution.test.ts, tests/integration/execution-quality.test.ts]
---

<objective>
Create layer-to-layer integration tests verifying real component interactions.

Purpose: Test that adjacent layers communicate correctly without mocking internal boundaries.
Output: 20 integration tests covering Infrastructure↔Persistence, Persistence↔Planning, Planning↔Execution, Execution↔Quality.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/11-integration-testing/11-CONTEXT.md
@.planning/phases/11-integration-testing/11-RESEARCH.md
@.planning/phases/11-integration-testing/11-01-SUMMARY.md

**Layer architecture (from phases 1-4):**
- Infrastructure: FileSystemService, GitService, WorktreeManager
- Persistence: Database, StateManager, RequirementsDB, CheckpointManager
- Planning: TaskDecomposer, DependencyResolver, TimeEstimator
- Execution: AgentPool, TaskQueue, EventBus
- Quality: QALoopEngine, BuildVerifier, LintRunner, TestRunner

**Key source files:**
@src/infrastructure/file-system/FileSystemService.ts
@src/persistence/database.ts
@src/planning/decomposition/TaskDecomposer.ts
@src/orchestration/agents/AgentPool.ts
@src/execution/qa-loop/QALoopEngine.ts

**From RESEARCH.md - Anti-patterns to avoid:**
- Mocking too much (test real layer interactions)
- Shared mutable state (use fixtures with cleanup)
- Testing implementation details (test behavior)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Infrastructure ↔ Persistence integration tests</name>
  <files>tests/integration/infra-persistence.test.ts</files>
  <action>
Create 5 integration tests verifying FileSystem and Database work together:

1. `should persist file operations to database` - FileSystemService writes file, StateManager records it
2. `should track git operations in state` - GitService commits, StateManager updates lastCommit
3. `should sync worktree state with database` - WorktreeManager creates worktree, DB records it
4. `should handle file deletion with state cleanup` - Delete file, verify state updated
5. `should recover state from disk after restart` - Write state, reinitialize, verify consistency

Use real FileSystemService with temp directory (cleaned up in afterEach).
Use real Database with in-memory SQLite from fixtures.
Do NOT mock layer boundaries - test actual integration.
  </action>
  <verify>npm test -- --run tests/integration/infra-persistence.test.ts (5 tests pass)</verify>
  <done>5 tests pass, covering file→DB sync, git→state updates, worktree tracking</done>
</task>

<task type="auto">
  <name>Task 2: Persistence ↔ Planning integration tests</name>
  <files>tests/integration/persistence-planning.test.ts</files>
  <action>
Create 5 integration tests verifying Database feeds Planning correctly:

1. `should decompose tasks from stored requirements` - RequirementsDB has requirements, TaskDecomposer reads and decomposes
2. `should resolve dependencies using stored task data` - Tasks in DB, DependencyResolver calculates waves
3. `should estimate time based on historical data` - Past task times in DB, TimeEstimator calibrates
4. `should persist decomposition results` - TaskDecomposer output saved to DB
5. `should handle empty requirements gracefully` - No requirements, decomposer returns empty

Use real RequirementsDB with test database from fixtures.
Use real TaskDecomposer, DependencyResolver, TimeEstimator.
Mock only LLM calls (MSW handles Claude/Gemini).
  </action>
  <verify>npm test -- --run tests/integration/persistence-planning.test.ts (5 tests pass)</verify>
  <done>5 tests pass, covering requirements→tasks, dependency resolution, time estimation</done>
</task>

<task type="auto">
  <name>Task 3: Planning ↔ Execution and Execution ↔ Quality integration tests</name>
  <files>tests/integration/planning-execution.test.ts, tests/integration/execution-quality.test.ts</files>
  <action>
**planning-execution.test.ts** (5 tests):
1. `should queue decomposed tasks for execution` - TaskDecomposer output → TaskQueue
2. `should assign tasks to agents respecting dependencies` - DependencyResolver waves → AgentPool assignment
3. `should emit events when tasks transition states` - TaskQueue state change → EventBus events
4. `should handle agent failure with task requeue` - Agent fails → task back in queue
5. `should coordinate multiple agents on parallel tasks` - Parallel wave → multiple agents run

**execution-quality.test.ts** (5 tests):
1. `should run QA loop on agent output` - Agent completes → QALoopEngine validates
2. `should trigger build verification after code changes` - Code written → BuildVerifier runs
3. `should run tests after build passes` - Build passes → TestRunner runs
4. `should fail QA loop on test failure` - Tests fail → loop reports failure
5. `should emit quality events through EventBus` - QA steps → events emitted

Use real components with MSW mocking LLM APIs.
Use fixtures for EventBus (to capture events).
  </action>
  <verify>npm test -- --run tests/integration/planning-execution.test.ts tests/integration/execution-quality.test.ts (10 tests pass)</verify>
  <done>10 tests pass, covering task queuing, agent coordination, QA loop integration</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm test -- --run tests/integration/` passes all 20 tests
- [ ] No flaky tests (run 3 times)
- [ ] Tests use real layer interactions (not over-mocked)
- [ ] Each test cleans up properly (no state leakage)
</verification>

<success_criteria>

- 20 layer integration tests passing
- Tests verify real component interactions
- MSW mocks only external APIs (Claude, Gemini)
- Fixtures handle setup/teardown cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/11-integration-testing/11-02-SUMMARY.md`
</output>
