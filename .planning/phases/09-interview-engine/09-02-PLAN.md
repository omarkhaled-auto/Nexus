---
phase: 09-interview-engine
plan: 02
type: execute
depends_on: ["09-01"]
files_modified: [src/interview/QuestionGenerator.ts, src/interview/QuestionGenerator.test.ts, src/interview/InterviewEngine.ts, src/interview/InterviewEngine.test.ts, src/interview/prompts/interviewer.ts, src/interview/prompts/extractor.ts]
---

<objective>
Build QuestionGenerator and InterviewEngine core - the orchestration layer for interview conversations.

Purpose: QuestionGenerator produces contextual follow-up questions and gap detection. InterviewEngine orchestrates the full interview flow: message processing, extraction, storage, and event emission.
Output: Working interview orchestration with ~25 tests total.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-interview-engine/09-CONTEXT.md
@.planning/phases/09-interview-engine/09-RESEARCH.md
@.planning/phases/09-interview-engine/09-01-SUMMARY.md

# Source files to reference
@src/interview/RequirementExtractor.ts
@src/interview/types.ts
@src/llm/clients/ClaudeClient.ts
@src/persistence/requirements/RequirementsDB.ts
@src/orchestration/event-bus/EventBus.ts
@src/renderer/src/stores/interviewStore.ts

**Tech stack available:** ClaudeClient, RequirementsDB, EventBus, RequirementExtractor (from 09-01)
**Established patterns:** AgentRunner tool loop, XML tag prompting, event emission

**From CONTEXT.md - Interview flow:**
```
User message â†’ InterviewEngine.processMessage()
  1. Add message to session
  2. Call QuestionGenerator.generate()
  3. Stream Claude response
  4. Call RequirementExtractor.extract()
  5. Store requirements in DB
  6. Update exploredAreas
  7. Check if should suggest gaps
  8. Emit interview:message event
```

**From RESEARCH.md - Gap detection:**
```typescript
const STANDARD_AREAS = [
  'authentication', 'authorization', 'data_model', 'api',
  'ui_ux', 'performance', 'security', 'integrations', 'deployment'
];
```
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create interview prompts and QuestionGenerator</name>
  <files>src/interview/prompts/interviewer.ts, src/interview/prompts/extractor.ts, src/interview/QuestionGenerator.ts, src/interview/QuestionGenerator.test.ts</files>
  <action>
    **1. Create prompts directory and files:**

    **interviewer.ts** - System prompt for the interview AI:
    - Role: Expert requirements analyst conducting discovery interview
    - Behavior: Ask clarifying questions, suggest gaps, summarize periodically
    - Progressive depth: Start broad, go detailed per area
    - Output format: Natural conversation + XML requirement tags when extracting
    - Include the extraction format from RESEARCH.md

    **extractor.ts** - Standalone extraction prompt:
    - For post-hoc extraction from conversation history
    - Same XML format as inline extraction

    **2. QuestionGenerator class:**
    ```typescript
    interface QuestionGeneratorOptions {
      llmClient: LLMClient;
      logger?: Logger;
    }

    interface GeneratedQuestion {
      question: string;
      area: string;
      depth: 'broad' | 'detailed' | 'clarifying';
      followsUp?: string;
    }

    interface GenerationContext {
      conversationHistory: InterviewMessage[];
      extractedRequirements: ExtractedRequirement[];
      exploredAreas: string[];
    }
    ```

    Methods:
    - `generate(context: GenerationContext): Promise<GeneratedQuestion>`
    - `detectGaps(exploredAreas: string[]): string[]` - Returns unexplored standard areas
    - `shouldSuggestGap(context: GenerationContext): boolean` - True if 3+ requirements and gaps exist
    - `getSystemPrompt(): string` - Returns interviewer system prompt

    **Tests (~10):**
    - generate() returns question with area and depth
    - detectGaps() returns unexplored areas
    - detectGaps() returns empty when all explored
    - shouldSuggestGap() returns true when conditions met
    - shouldSuggestGap() returns false when too few requirements
    - getSystemPrompt() returns non-empty prompt
    - Mock LLM client for testing
  </action>
  <verify>npm test -- src/interview/QuestionGenerator.test.ts</verify>
  <done>QuestionGenerator with 10 passing tests, prompts created</done>
</task>

<task type="auto">
  <name>Task 2: Create InterviewEngine core</name>
  <files>src/interview/InterviewEngine.ts, src/interview/InterviewEngine.test.ts, src/interview/index.ts</files>
  <action>
    **InterviewEngine class - main orchestrator:**
    ```typescript
    interface InterviewEngineOptions {
      llmClient: LLMClient;
      requirementsDB: RequirementsDB;
      eventBus: EventBus;
      logger?: Logger;
    }

    interface InterviewSession {
      id: string;
      projectId: string;
      status: 'active' | 'paused' | 'completed';
      messages: InterviewMessage[];
      extractedRequirements: ExtractedRequirement[];
      exploredAreas: string[];
      startedAt: Date;
      lastActivityAt: Date;
      completedAt?: Date;
    }

    interface ProcessMessageResult {
      response: string;
      extractedRequirements: ExtractedRequirement[];
      suggestedGaps: string[];
    }
    ```

    **Methods:**
    - `startSession(projectId: string): InterviewSession` - Creates new session
    - `processMessage(sessionId: string, userMessage: string): Promise<ProcessMessageResult>` - Main flow
    - `getSession(sessionId: string): InterviewSession | null`
    - `endSession(sessionId: string): void` - Marks completed
    - `pauseSession(sessionId: string): void` - Marks paused

    **processMessage flow (from CONTEXT.md):**
    1. Add user message to session
    2. Build messages array with system prompt + history
    3. Call Claude via llmClient.chat()
    4. Add assistant response to session
    5. Call RequirementExtractor.extract()
    6. Store extracted requirements in RequirementsDB
    7. Update exploredAreas from requirement areas
    8. Check shouldSuggestGap() and include in response
    9. Emit 'interview:message' event
    10. Return response + extracted + gaps

    **Events emitted:**
    - 'interview:started' - { sessionId, projectId }
    - 'interview:message' - { sessionId, messageId, role, content }
    - 'interview:requirement' - { sessionId, requirementId, text, category }
    - 'interview:completed' - { sessionId, requirementCount }

    **Tests (~15):**
    - startSession() creates session with correct fields
    - startSession() emits interview:started event
    - processMessage() adds user message to session
    - processMessage() calls LLM with correct messages
    - processMessage() extracts requirements from response
    - processMessage() stores requirements in DB
    - processMessage() updates exploredAreas
    - processMessage() emits interview:message event
    - processMessage() emits interview:requirement for each extracted
    - processMessage() returns suggestedGaps when appropriate
    - getSession() returns existing session
    - getSession() returns null for unknown session
    - endSession() marks status completed
    - endSession() emits interview:completed event
    - pauseSession() marks status paused

    **Create index.ts barrel export:**
    ```typescript
    export * from './types';
    export * from './RequirementExtractor';
    export * from './QuestionGenerator';
    export * from './InterviewEngine';
    export * from './prompts/interviewer';
    export * from './prompts/extractor';
    ```
  </action>
  <verify>npm test -- src/interview/InterviewEngine.test.ts && npm test -- src/interview/</verify>
  <done>InterviewEngine with 15 tests, full interview flow working, events emitted correctly</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm test -- src/interview/` passes all tests (~35 including 09-01)
- [ ] `npm run build` succeeds without errors
- [ ] QuestionGenerator generates contextual questions
- [ ] InterviewEngine processes messages through full flow
- [ ] Events emitted correctly for all lifecycle events
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- ~25 new tests added (10 QuestionGenerator + 15 InterviewEngine)
- Interview flow works end-to-end (mocked LLM)
- Events integrate with EventBus
</success_criteria>

<output>
After completion, create `.planning/phases/09-interview-engine/09-02-SUMMARY.md`:

# Plan 09-02: QuestionGenerator + InterviewEngine Summary

**[One-liner: what shipped]**

## Accomplishments

- [Key outcomes]

## Files Created/Modified

- `src/interview/prompts/interviewer.ts` - Interview system prompt
- `src/interview/prompts/extractor.ts` - Extraction prompt
- `src/interview/QuestionGenerator.ts` - Question generation and gap detection
- `src/interview/QuestionGenerator.test.ts` - 10 tests
- `src/interview/InterviewEngine.ts` - Main orchestrator
- `src/interview/InterviewEngine.test.ts` - 15 tests
- `src/interview/index.ts` - Barrel export

## Decisions Made

[Key decisions and rationale]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 09-03-PLAN.md (InterviewSessionManager + IPC)
</output>
